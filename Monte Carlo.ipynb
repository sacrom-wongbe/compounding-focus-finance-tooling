{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed8841f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip uninstall cupy-cuda12x cupy -y\n",
    "%pip install cupy-cuda12x --no-cache-dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bd52a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "cuda_bin = r\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.6\\bin\"\n",
    "nvrtc_dlls = glob.glob(os.path.join(cuda_bin, \"nvrtc*.dll\"))\n",
    "\n",
    "print(\"Found NVRTC DLLs:\")\n",
    "for dll in nvrtc_dlls:\n",
    "    print(f\"  - {os.path.basename(dll)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c5fad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Add both bin and lib folders\n",
    "cuda_base = r\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.6\"\n",
    "cuda_bin = os.path.join(cuda_base, \"bin\")\n",
    "cuda_lib = os.path.join(cuda_base, \"lib\", \"x64\")\n",
    "\n",
    "# Add to PATH\n",
    "os.environ['PATH'] = cuda_bin + os.pathsep + cuda_lib + os.pathsep + os.environ['PATH']\n",
    "print(f\"✓ Added CUDA bin and lib to PATH\")\n",
    "\n",
    "# Also set CUDA_PATH\n",
    "os.environ['CUDA_PATH'] = cuda_base\n",
    "print(f\"✓ Set CUDA_PATH\")\n",
    "\n",
    "# Now try CuPy\n",
    "import cupy as cp\n",
    "x = cp.random.random((1000, 1000))\n",
    "y = cp.sum(x)\n",
    "print(f\"✓ GPU computation successful: {float(y):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c3df8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "print(cp.__version__)  # Should print: 13.6.0\n",
    "print(cp.cuda.runtime.getDeviceCount())  # Should show your GPU count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c707fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "try:\n",
    "    result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)\n",
    "    print(result.stdout)\n",
    "except FileNotFoundError:\n",
    "    print(\"❌ nvidia-smi not found - checking if you have NVIDIA GPU...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb9aaee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  20 of 20 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running with ThreadPoolExecutor (10 threads)...\n",
      "✓ Threading completed (10000 simulations).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h2>Compounding Safety Analysis (30-Year Monte Carlo Simulation)</h2>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Metric</th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>CAGR (Mean)</td>\n",
       "      <td>16.15%</td>\n",
       "      <td>14.20%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>CAGR (StdDev)</td>\n",
       "      <td>3.12%</td>\n",
       "      <td>3.07%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Volatility Annual (Mean)</td>\n",
       "      <td>16.32%</td>\n",
       "      <td>16.34%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Sharpe Ratio (Mean)</td>\n",
       "      <td>0.838</td>\n",
       "      <td>0.718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Sortino Ratio (Mean)</td>\n",
       "      <td>1.215</td>\n",
       "      <td>1.028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Win Rate % (Mean)</td>\n",
       "      <td>53.6%</td>\n",
       "      <td>53.7%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Positive Years % (Mean)</td>\n",
       "      <td>84.4%</td>\n",
       "      <td>81.5%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Recovery Factor (Mean)</td>\n",
       "      <td>444.08x</td>\n",
       "      <td>248.53x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Decade 1 CAGR (Mean)</td>\n",
       "      <td>16.21%</td>\n",
       "      <td>14.27%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Decade 2 CAGR (Mean)</td>\n",
       "      <td>16.24%</td>\n",
       "      <td>14.28%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Decade 3 CAGR (Mean)</td>\n",
       "      <td>16.26%</td>\n",
       "      <td>14.28%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Probability of Ruin (%)</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>0.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Max Drawdown (Mean)</td>\n",
       "      <td>-30.51%</td>\n",
       "      <td>-32.56%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Max Drawdown (StdDev)</td>\n",
       "      <td>6.22%</td>\n",
       "      <td>6.84%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Total Growth (Mean)</td>\n",
       "      <td>122.37x</td>\n",
       "      <td>73.14x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Total Growth (5th %ile)</td>\n",
       "      <td>23.13x</td>\n",
       "      <td>14.16x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Growth $ from $10k (5th %ile)</td>\n",
       "      <td>$231,283</td>\n",
       "      <td>$141,572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Martin Ratio (Mean)</td>\n",
       "      <td>15.9771</td>\n",
       "      <td>8.6347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Efficiency Score (Mean)</td>\n",
       "      <td>0.3076</td>\n",
       "      <td>0.1643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Probability of 5-Year Underwater Streak</td>\n",
       "      <td>9.5%</td>\n",
       "      <td>17.7%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Max Underwater Streak (Mean Days)</td>\n",
       "      <td>787</td>\n",
       "      <td>928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Max Underwater Streak (Mean Years)</td>\n",
       "      <td>3.1</td>\n",
       "      <td>3.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Total Underwater Days</td>\n",
       "      <td>6729</td>\n",
       "      <td>6805</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display, HTML\n",
    "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor\n",
    "import multiprocessing\n",
    "\n",
    "# Optional GPU acceleration (CuPy); set use_gpu=True below to attempt\n",
    "try:\n",
    "    import cupy as cp  # Use the installed binary\n",
    "    _has_cupy = True\n",
    "except ImportError:\n",
    "    try:\n",
    "        import cupy as cp  # Fallback\n",
    "        _has_cupy = True\n",
    "    except ImportError:\n",
    "        cp = None\n",
    "        _has_cupy = False\n",
    "\n",
    "# Global variables for worker processes\n",
    "_returns_matrix = None\n",
    "_ordered_cols = None\n",
    "_years = None\n",
    "_block_size = None\n",
    "\n",
    "def init_worker(returns_mat, cols, yrs, blk_sz):\n",
    "    \"\"\"Initialize each worker process with shared data\"\"\"\n",
    "    global _returns_matrix, _ordered_cols, _years, _block_size\n",
    "    _returns_matrix = returns_mat\n",
    "    _ordered_cols = cols\n",
    "    _years = yrs\n",
    "    _block_size = blk_sz\n",
    "\n",
    "# 1. Setup & Data Fetching\n",
    "# Tickers ranked by performance metrics (from best to worst)\n",
    "tickers = ['USMV', 'SCHD', 'VDC', 'VOO', 'QUAL', 'SPMO', 'AVDV', 'XLP',\n",
    "           'VUG', 'XLV', 'VTI', 'AVUV', 'VXUS', 'QQQM', 'SPGP', 'VT',\n",
    "           'JQUA', 'DYNF', 'AVDE', 'AVEM']\n",
    "\n",
    "# Download data\n",
    "raw_data = yf.download(tickers, start=\"2020-01-01\", end=\"2025-01-01\")\n",
    "\n",
    "# Handle Adj Close fallback - use 'Adj Close' if available, otherwise 'Close'\n",
    "if 'Adj Close' in raw_data.columns.get_level_values(0):\n",
    "    data = raw_data['Adj Close']\n",
    "else:\n",
    "    # If multi-level columns but no Adj Close\n",
    "    if isinstance(raw_data.columns, pd.MultiIndex):\n",
    "        if 'Close' in raw_data.columns.get_level_values(0):\n",
    "            data = raw_data['Close']\n",
    "        else:\n",
    "            raise ValueError(\"Neither 'Adj Close' nor 'Close' found in data\")\n",
    "    else:\n",
    "        data = raw_data\n",
    "\n",
    "returns = data.pct_change().dropna()\n",
    "\n",
    "# 2. Define Your Two Portfolios\n",
    "# Portfolio A {'SPMO': 0.1, 'VTI': 0.3, 'VONG': 0.1, 'VXUS': 0.075, 'AVUV': 0.1, 'AVDV': 0.075, 'JQUA': 0.1, 'DYNF': 0.15}\n",
    "weights_a = {'SPMO': 0.1, 'VTI': 0.4, 'AVUV': 0.1, 'AVDV': 0.1, 'DYNF': 0.2, 'AVDE': 0.075, 'AVEM': 0.025}\n",
    "# Portfolio B\n",
    "weights_b = {'VTI': 0.85, 'VXUS': 0.15}\n",
    "\n",
    "# Simulation controls\n",
    "n_sims = 10000  # increase/decrease to control number of Monte Carlo paths\n",
    "block_size = 20\n",
    "years = 30\n",
    "year_streak = 5\n",
    "use_gpu = False  # set True to attempt CuPy GPU path (serial execution)\n",
    "\n",
    "# Optimal worker count: 4-6 workers is the sweet spot for most systems\n",
    "max_workers = 10  # Conservative, reliable setting\n",
    "use_processes = False  # True = ProcessPoolExecutor (separate processes)\n",
    "use_threads = True # True = ThreadPoolExecutor (threads in same process)\n",
    "\n",
    "# Precompute structures for faster simulation\n",
    "ordered_cols = list(returns.columns)\n",
    "returns_matrix_np = returns.to_numpy()\n",
    "\n",
    "# Choose array module\n",
    "xp = cp if (use_gpu and _has_cupy) else np\n",
    "if use_gpu and not _has_cupy:\n",
    "    print(\"CuPy not available; falling back to CPU (NumPy).\")\n",
    "\n",
    "# Build weight vectors aligned to returns columns\n",
    "def build_weight_vector(weights, xp_mod):\n",
    "    return xp_mod.array([weights.get(col, 0.0) for col in ordered_cols], dtype=float)\n",
    "\n",
    "weights_a_vec = build_weight_vector(weights_a, xp)\n",
    "weights_b_vec = build_weight_vector(weights_b, xp)\n",
    "\n",
    "# If using GPU, move returns to device once\n",
    "returns_matrix_gpu = xp.asarray(returns_matrix_np) if (xp is cp) else None\n",
    "\n",
    "def simulate_future(returns_matrix, weights_vec, years=30, block_size=20, xp_mod=np):\n",
    "    \"\"\"Creates a 'Frankenstein' future by stitching random blocks (CPU or GPU).\"\"\"\n",
    "    n_days = years * 252\n",
    "    sim_blocks = []\n",
    "    total_len = 0\n",
    "\n",
    "    while total_len < n_days:\n",
    "        start_idx = int(xp_mod.random.randint(0, returns_matrix.shape[0] - block_size))\n",
    "        block = returns_matrix[start_idx : start_idx + block_size]\n",
    "        port_block = xp_mod.sum(block * weights_vec, axis=1)\n",
    "        sim_blocks.append(port_block)\n",
    "        total_len += block_size\n",
    "\n",
    "    sim_returns = xp_mod.concatenate(sim_blocks)[:n_days]\n",
    "    wealth_index = xp_mod.cumprod(1 + sim_returns)\n",
    "    # Move back to CPU numpy for downstream pandas/metrics\n",
    "    wealth_index_cpu = wealth_index.get() if xp_mod is cp else wealth_index\n",
    "    return pd.Series(wealth_index_cpu)\n",
    "\n",
    "def calculate_metrics(path):\n",
    "    \"\"\"Calculate comprehensive metrics for a given path.\"\"\"\n",
    "    # Max Drawdown\n",
    "    dd = (path / path.cummax() - 1).min()\n",
    "\n",
    "    # Drawdown area (days below cummax times magnitude)\n",
    "    drawdowns = path / path.cummax() - 1\n",
    "    drawdown_area = (drawdowns[drawdowns < 0]).abs().sum()\n",
    "\n",
    "    # Ulcer Index (sqrt of average squared drawdowns)\n",
    "    ulcer_index = np.sqrt((drawdowns[drawdowns < 0] ** 2).sum() / len(path)) * 100\n",
    "\n",
    "    # Final Growth\n",
    "    growth = path.iloc[-1]\n",
    "\n",
    "    # CAGR (years assumed)\n",
    "    cagr = (growth ** (1 / years)) - 1\n",
    "\n",
    "    # Daily returns volatility\n",
    "    daily_returns = path.pct_change().dropna()\n",
    "    volatility_annual = daily_returns.std() * np.sqrt(252)\n",
    "\n",
    "    # Sharpe Ratio (assuming 2.5% risk-free rate)\n",
    "    risk_free_rate = 0.025\n",
    "    sharpe_ratio = (cagr - risk_free_rate) / volatility_annual if volatility_annual > 0 else 0\n",
    "\n",
    "    # Sortino Ratio (only downside volatility)\n",
    "    downside_returns = daily_returns[daily_returns < 0]\n",
    "    downside_volatility = downside_returns.std() * np.sqrt(252) if len(downside_returns) > 0 else 0\n",
    "    sortino_ratio = (cagr - risk_free_rate) / downside_volatility if downside_volatility > 0 else 0\n",
    "\n",
    "    # Win Rate (% of days with positive returns)\n",
    "    win_rate = (daily_returns > 0).sum() / len(daily_returns) * 100 if len(daily_returns) > 0 else 0\n",
    "\n",
    "    # Positive Years % - count how many years had positive returns\n",
    "    yearly_returns = []\n",
    "    for year_idx in range(years):\n",
    "        start_idx = year_idx * 252\n",
    "        end_idx = (year_idx + 1) * 252\n",
    "        if end_idx < len(path):\n",
    "            year_return = (path.iloc[end_idx] / path.iloc[start_idx]) - 1\n",
    "            yearly_returns.append(year_return)\n",
    "    positive_years_pct = (sum(1 for r in yearly_returns if r > 0) / len(yearly_returns) * 100) if yearly_returns else 0\n",
    "\n",
    "    # Recovery Factor = Total Return / Max Drawdown magnitude\n",
    "    recovery_factor = (growth - 1) / abs(dd) if dd != 0 else 0\n",
    "\n",
    "    # Decade CAGRs\n",
    "    decade_cagrs = []\n",
    "    for decade_start in [0, 10, 20]:\n",
    "        start_idx = decade_start * 252\n",
    "        end_idx = min((decade_start + 10) * 252, len(path) - 1)\n",
    "        if end_idx > start_idx and start_idx < len(path) and end_idx < len(path):\n",
    "            decade_growth = path.iloc[end_idx] / path.iloc[start_idx]\n",
    "            decade_cagr = (decade_growth ** (1 / 10)) - 1 if decade_growth > 0 else 0\n",
    "            decade_cagrs.append(decade_cagr)\n",
    "        else:\n",
    "            decade_cagrs.append(0)\n",
    "\n",
    "    # Martin Ratio = Return / Ulcer Index\n",
    "    martin_ratio = (growth - 1) / ulcer_index if ulcer_index > 0 else 0\n",
    "\n",
    "    # Compounding Efficiency Score = Total Return / Drawdown Area\n",
    "    efficiency = (growth - 1) / drawdown_area if drawdown_area > 0 else 0\n",
    "\n",
    "    # Check for x-year underwater streak (x-years * 252)\n",
    "    underwater = (path < path.cummax()).astype(int)\n",
    "    streaks = []\n",
    "    current_streak = 0\n",
    "    for is_underwater in underwater:\n",
    "        if is_underwater:\n",
    "            current_streak += 1\n",
    "        else:\n",
    "            if current_streak > 0:\n",
    "                streaks.append(current_streak)\n",
    "            current_streak = 0\n",
    "    if current_streak > 0:\n",
    "        streaks.append(current_streak)\n",
    "\n",
    "    max_streak = max(streaks) if streaks else 0\n",
    "    has_xyr_streak = max_streak >= 252 * year_streak  # x years * 252 trading days\n",
    "\n",
    "    # Total underwater days\n",
    "    total_underwater_days = (underwater == 1).sum()\n",
    "\n",
    "    return {\n",
    "        'DD': dd,\n",
    "        'Growth': growth,\n",
    "        'CAGR': cagr,\n",
    "        'Volatility_Annual': volatility_annual,\n",
    "        'Sharpe_Ratio': sharpe_ratio,\n",
    "        'Sortino_Ratio': sortino_ratio,\n",
    "        'Win_Rate': win_rate,\n",
    "        'Positive_Years_Pct': positive_years_pct,\n",
    "        'Recovery_Factor': recovery_factor,\n",
    "        'Decade1_CAGR': decade_cagrs[0] if len(decade_cagrs) > 0 else 0,\n",
    "        'Decade2_CAGR': decade_cagrs[1] if len(decade_cagrs) > 1 else 0,\n",
    "        'Decade3_CAGR': decade_cagrs[2] if len(decade_cagrs) > 2 else 0,\n",
    "        'Drawdown_Area': drawdown_area,\n",
    "        'Ulcer_Index': ulcer_index,\n",
    "        'Martin_Ratio': martin_ratio,\n",
    "        'Efficiency_Score': efficiency,\n",
    "        f'Has_{year_streak}Yr_Streak': has_xyr_streak,\n",
    "        'Max_Underwater_Streak_Days': max_streak,\n",
    "        'Total_Underwater_Days': total_underwater_days\n",
    "    }\n",
    "\n",
    "# Helper for parallel execution\n",
    "def _run_single_sim_cpu(weights_dict):\n",
    "    \"\"\"Run single simulation in worker process using global data\"\"\"\n",
    "    global _returns_matrix, _ordered_cols, _years, _block_size\n",
    "    weights_vec = np.array([weights_dict.get(col, 0.0) for col in _ordered_cols], dtype=float)\n",
    "    path = simulate_future(_returns_matrix, weights_vec, years=_years, block_size=_block_size, xp_mod=np)\n",
    "    return calculate_metrics(path)\n",
    "\n",
    "# Execute simulations (CPU threading/multiprocessing or single-process GPU)\n",
    "if __name__ == '__main__':\n",
    "    if xp is cp:\n",
    "        print(\"Running on GPU (serial execution)...\")\n",
    "        results_a = [calculate_metrics(simulate_future(returns_matrix_gpu, weights_a_vec, years=years, block_size=block_size, xp_mod=cp)) for _ in range(n_sims)]\n",
    "        results_b = [calculate_metrics(simulate_future(returns_matrix_gpu, weights_b_vec, years=years, block_size=block_size, xp_mod=cp)) for _ in range(n_sims)]\n",
    "        print(f\"✓ GPU execution completed ({len(results_a)} simulations).\")\n",
    "    else:\n",
    "        # CPU path\n",
    "        if use_processes:\n",
    "            # Calculate optimal chunksize: larger chunks = less overhead\n",
    "            optimal_chunksize = max(n_sims // (4 * max_workers), 100)\n",
    "            \n",
    "            print(f\"Running with ProcessPoolExecutor ({max_workers} workers, chunksize={optimal_chunksize})...\")\n",
    "            \n",
    "            with ProcessPoolExecutor(\n",
    "                max_workers=max_workers,\n",
    "                mp_context=multiprocessing.get_context(\"spawn\"),\n",
    "                initializer=init_worker,\n",
    "                initargs=(returns_matrix_np, ordered_cols, years, block_size)\n",
    "            ) as executor:\n",
    "                results_a = list(executor.map(_run_single_sim_cpu, [weights_a] * n_sims, \n",
    "                                             chunksize=optimal_chunksize))\n",
    "                results_b = list(executor.map(_run_single_sim_cpu, [weights_b] * n_sims,\n",
    "                                             chunksize=optimal_chunksize))\n",
    "            \n",
    "            print(f\"✓ Multiprocessing completed ({len(results_a)} simulations).\")\n",
    "        elif use_threads:\n",
    "            print(f\"Running with ThreadPoolExecutor ({max_workers or multiprocessing.cpu_count()} threads)...\")\n",
    "            weights_a_vec_np = np.array([weights_a.get(col, 0.0) for col in ordered_cols], dtype=float)\n",
    "            weights_b_vec_np = np.array([weights_b.get(col, 0.0) for col in ordered_cols], dtype=float)\n",
    "            \n",
    "            def _run_single_sim_cpu_thread(weights_vec_np):\n",
    "                path = simulate_future(returns_matrix_np, weights_vec_np, years=years, block_size=block_size, xp_mod=np)\n",
    "                return calculate_metrics(path)\n",
    "            \n",
    "            with ThreadPoolExecutor(max_workers=max_workers or multiprocessing.cpu_count()) as executor:\n",
    "                results_a = list(executor.map(_run_single_sim_cpu_thread, [weights_a_vec_np] * n_sims))\n",
    "                results_b = list(executor.map(_run_single_sim_cpu_thread, [weights_b_vec_np] * n_sims))\n",
    "            print(f\"✓ Threading completed ({len(results_a)} simulations).\")\n",
    "        else:\n",
    "            print(\"Running serially (no parallelism)...\")\n",
    "            weights_a_vec_np = np.array([weights_a.get(col, 0.0) for col in ordered_cols], dtype=float)\n",
    "            weights_b_vec_np = np.array([weights_b.get(col, 0.0) for col in ordered_cols], dtype=float)\n",
    "            \n",
    "            def _run_single_sim_cpu_serial(weights_vec_np):\n",
    "                path = simulate_future(returns_matrix_np, weights_vec_np, years=years, block_size=block_size, xp_mod=np)\n",
    "                return calculate_metrics(path)\n",
    "            \n",
    "            results_a = [_run_single_sim_cpu_serial(weights_a_vec_np) for _ in range(n_sims)]\n",
    "            results_b = [_run_single_sim_cpu_serial(weights_b_vec_np) for _ in range(n_sims)]\n",
    "            print(f\"✓ Serial execution completed ({len(results_a)} simulations).\")\n",
    "\n",
    "    # 4. Analyze the \"Compounding Safety\"\n",
    "    df_a = pd.DataFrame(results_a)\n",
    "    df_b = pd.DataFrame(results_b)\n",
    "\n",
    "    # Calculate Probability of Ruin (% of sims where final growth < 1.0)\n",
    "    prob_ruin_a = (df_a['Growth'] < 1.0).sum() / len(df_a) * 100\n",
    "    prob_ruin_b = (df_b['Growth'] < 1.0).sum() / len(df_b) * 100\n",
    "\n",
    "    # Create summary statistics\n",
    "    summary_data = {\n",
    "        'Metric': [\n",
    "            'CAGR (Mean)',\n",
    "            'CAGR (StdDev)',\n",
    "            'Volatility Annual (Mean)',\n",
    "            'Sharpe Ratio (Mean)',\n",
    "            'Sortino Ratio (Mean)',\n",
    "            'Win Rate % (Mean)',\n",
    "            'Positive Years % (Mean)',\n",
    "            'Recovery Factor (Mean)',\n",
    "            'Decade 1 CAGR (Mean)',\n",
    "            'Decade 2 CAGR (Mean)',\n",
    "            'Decade 3 CAGR (Mean)',\n",
    "            'Probability of Ruin (%)',\n",
    "            'Max Drawdown (Mean)',\n",
    "            'Max Drawdown (StdDev)',\n",
    "            'Total Growth (Mean)',\n",
    "            'Total Growth (5th %ile)',\n",
    "            'Growth $ from $10k (5th %ile)',\n",
    "            'Martin Ratio (Mean)',\n",
    "            'Efficiency Score (Mean)',\n",
    "            f'Probability of {year_streak}-Year Underwater Streak',\n",
    "            'Max Underwater Streak (Mean Days)',\n",
    "            'Max Underwater Streak (Mean Years)',\n",
    "            'Total Underwater Days'\n",
    "        ],\n",
    "        'A': [\n",
    "            f\"{df_a['CAGR'].mean():.2%}\",\n",
    "            f\"{df_a['CAGR'].std():.2%}\",\n",
    "            f\"{df_a['Volatility_Annual'].mean():.2%}\",\n",
    "            f\"{df_a['Sharpe_Ratio'].mean():.3f}\",\n",
    "            f\"{df_a['Sortino_Ratio'].mean():.3f}\",\n",
    "            f\"{df_a['Win_Rate'].mean():.1f}%\",\n",
    "            f\"{df_a['Positive_Years_Pct'].mean():.1f}%\",\n",
    "            f\"{df_a['Recovery_Factor'].mean():.2f}x\",\n",
    "            f\"{df_a['Decade1_CAGR'].mean():.2%}\",\n",
    "            f\"{df_a['Decade2_CAGR'].mean():.2%}\",\n",
    "            f\"{df_a['Decade3_CAGR'].mean():.2%}\",\n",
    "            f\"{prob_ruin_a:.1f}%\",\n",
    "            f\"{df_a['DD'].mean():.2%}\",\n",
    "            f\"{df_a['DD'].std():.2%}\",\n",
    "            f\"{df_a['Growth'].mean():.2f}x\",\n",
    "            f\"{df_a['Growth'].quantile(0.05):.2f}x\",\n",
    "            f\"${10000 * df_a['Growth'].quantile(0.05):,.0f}\",\n",
    "            f\"{df_a['Martin_Ratio'].mean():.4f}\",\n",
    "            f\"{df_a['Efficiency_Score'].mean():.4f}\",\n",
    "            f\"{df_a[f'Has_{year_streak}Yr_Streak'].sum() / len(df_a) * 100:.1f}%\",\n",
    "            f\"{df_a['Max_Underwater_Streak_Days'].mean():.0f}\",\n",
    "            f\"{df_a['Max_Underwater_Streak_Days'].mean() / 252:.1f}\",\n",
    "            f\"{df_a['Total_Underwater_Days'].mean():.0f}\"\n",
    "        ],\n",
    "        'B': [\n",
    "            f\"{df_b['CAGR'].mean():.2%}\",\n",
    "            f\"{df_b['CAGR'].std():.2%}\",\n",
    "            f\"{df_b['Volatility_Annual'].mean():.2%}\",\n",
    "            f\"{df_b['Sharpe_Ratio'].mean():.3f}\",\n",
    "            f\"{df_b['Sortino_Ratio'].mean():.3f}\",\n",
    "            f\"{df_b['Win_Rate'].mean():.1f}%\",\n",
    "            f\"{df_b['Positive_Years_Pct'].mean():.1f}%\",\n",
    "            f\"{df_b['Recovery_Factor'].mean():.2f}x\",\n",
    "            f\"{df_b['Decade1_CAGR'].mean():.2%}\",\n",
    "            f\"{df_b['Decade2_CAGR'].mean():.2%}\",\n",
    "            f\"{df_b['Decade3_CAGR'].mean():.2%}\",\n",
    "            f\"{prob_ruin_b:.1f}%\",\n",
    "            f\"{df_b['DD'].mean():.2%}\",\n",
    "            f\"{df_b['DD'].std():.2%}\",\n",
    "            f\"{df_b['Growth'].mean():.2f}x\",\n",
    "            f\"{df_b['Growth'].quantile(0.05):.2f}x\",\n",
    "            f\"${10000 * df_b['Growth'].quantile(0.05):,.0f}\",\n",
    "            f\"{df_b['Martin_Ratio'].mean():.4f}\",\n",
    "            f\"{df_b['Efficiency_Score'].mean():.4f}\",\n",
    "            f\"{df_b[f'Has_{year_streak}Yr_Streak'].sum() / len(df_b) * 100:.1f}%\",\n",
    "            f\"{df_b['Max_Underwater_Streak_Days'].mean():.0f}\",\n",
    "            f\"{df_b['Max_Underwater_Streak_Days'].mean() / 252:.1f}\",\n",
    "            f\"{df_b['Total_Underwater_Days'].mean():.0f}\"\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    display(HTML(\"<h2>Compounding Safety Analysis (30-Year Monte Carlo Simulation)</h2>\"))\n",
    "    display(HTML(summary_df.to_html(index=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f122a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. Define your 2026 Alpha Stack Weights\n",
    "weights = {\n",
    "    'VTI': 0.30,\n",
    "    'DYNF': 0.30,\n",
    "    'SPMO': 0.10,\n",
    "    'AVUV': 0.10,\n",
    "    'AVDV': 0.10,\n",
    "    'VXUS': 0.10\n",
    "}\n",
    "\n",
    "# 2. Define your \"Nightmare\" and \"Dream\" Scenarios (Annualized Estimates)\n",
    "# These aren't historical; they are your structural theses.\n",
    "scenarios = {\n",
    "    \"Japan Nightmare (Stagnation)\": {\n",
    "        'VTI': -0.02, 'DYNF': 0.01, 'SPMO': -0.05, 'AVUV': 0.02, 'AVDV': 0.04, 'VXUS': 0.03\n",
    "    },\n",
    "    \"Rentier Boom (Tech Monopoly)\": {\n",
    "        'VTI': 0.12, 'DYNF': 0.15, 'SPMO': 0.20, 'AVUV': 0.08, 'AVDV': 0.06, 'VXUS': 0.07\n",
    "    },\n",
    "    \"Inflationary Reset (70s Style)\": {\n",
    "        'VTI': 0.01, 'DYNF': 0.03, 'SPMO': -0.08, 'AVUV': 0.07, 'AVDV': 0.06, 'VXUS': 0.04\n",
    "    },\n",
    "    \"Global Bipolarity (US Crash / Intl Rally)\": {\n",
    "        'VTI': -0.05, 'DYNF': -0.02, 'SPMO': -0.10, 'AVUV': 0.01, 'AVDV': 0.08, 'VXUS': 0.06\n",
    "    }\n",
    "}\n",
    "\n",
    "# 3. Calculate Portfolio Performance per Regime\n",
    "results = {}\n",
    "for name, returns in scenarios.items():\n",
    "    port_return = sum(weights[ticker] * returns[ticker] for ticker in weights)\n",
    "    results[name] = f\"{port_return:.2%}\"\n",
    "\n",
    "# Output results\n",
    "df_results = pd.DataFrame.from_dict(results, orient='index', columns=['Projected Annual Return'])\n",
    "print(df_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bf030f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
